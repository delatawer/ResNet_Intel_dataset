# ResNet style network for Intel image dataset

## Assignment part of the Deep Learning Specialization

Residual Network: In order to solve the problem of the vanishing/exploding gradient, this architecture introduced the concept called Residual Blocks. In this network, we use a technique called skip connections. The skip connection connects activations of a  layer to further layers by skipping some layers in between. This forms a residual block. Resnets are made by stacking these residual blocks together. (https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/)

![imagen](https://user-images.githubusercontent.com/85259381/192029825-f5f16f57-2af1-4c8a-baf2-edbc37d7de50.png)
